# -*- coding: utf-8 -*-
"""books-recommandation-systems-machine-learning-terapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zx9EXNIeceZa6kfSWcYHGG7Y93lFHl0P

# Data Understanding

Mengimport library yang akan digunakan
"""

import pandas as pd
import numpy as np

from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Mengunduh dataset yang akan digunakan pada platform dataset kaggle"""

#!/bin/bash
!kaggle datasets download -d arashnic/book-recommendation-dataset

"""Mengekstrak dataset yang telah diunduh"""

!unzip book-recommendation-dataset.zip

"""Membaca file csv dan menyimpan ke dalam masing masing variabel df_books, df_ratings, df_users"""

df_books = pd.read_csv('Books.csv')
df_ratings = pd.read_csv('Ratings.csv')
df_users = pd.read_csv('Users.csv')

"""Menampilkan 5 data buku"""

df_books.head()

df_ratings.head()

df_users.head()

"""# Univariate Exploratory Data Analysis

Melihat informasi dataset seperti tipe data dan jumlah data
"""

df_books.info()

df_ratings.info()

df_users.info()

"""Menampilkan jumlah buku unik berdasarkan kolom isbn"""

print('Banyak buku: ', len(df_books.ISBN.unique()))

"""Menghitung jumlah penulis buku (authors)"""

df_books['Book-Author'].value_counts()

"""Menghitung jumlah publisher buku (Publisher)"""

df_books['Publisher'].value_counts()

"""Menghitung jumlah users yang memberikan rating (User_ID)"""

df_ratings['User-ID'].value_counts()

"""Menghitung jumlah rating pada buku yang diberikan oleh user (ISBN)"""

df_ratings['ISBN'].value_counts()

"""Menghitung jumlah masing masing nilai rating"""

df_ratings['Book-Rating'].value_counts()

"""# Data Preprocessing

Menggabungkan data buku dengan data rating buku
"""

# Menggabungkan seluruh ISBN pada buku
books_all = np.concatenate((
    df_books.ISBN.unique(),
    df_ratings.ISBN.unique(),
))

# Mengurutkan data dan menghapus data yang sama
books_all = np.sort(np.unique(books_all))

print('Jumlah seluruh data buku berdasarkan ISBN: ', len(books_all))

"""Menggabungkan Seluruh User"""

# Menggabungkan seluruh userID
users_all = np.concatenate((
    df_ratings['User-ID'].unique(),
    df_users['User-ID'].unique(),
))

# Menghapus data yang sama kemudian mengurutkannya
users_all = np.sort(np.unique(users_all))

print('Jumlah seluruh user: ', len(users_all))

"""Menggabungkan dataframe books dan ratings berdasarkan nilai ISBN"""

# Menggabungkan dataframe df_books dengan df_ratings berdasarkan nilai ISBN
books_merge = pd.merge(df_books, df_ratings , on='ISBN', how='inner')
books_merge

"""Menggabungkan dataframe books_merge dan users berdasarkan nilai User-ID"""

# Menggabungkan dataframe books_merge dengan df_users berdasarkan nilai User-ID
books = pd.merge(books_merge, df_users , on='User-ID', how='inner')
books

"""Memeriksa missing value"""

# Cek missing value dengan fungsi isnull()
books.isnull().sum()

"""# Data Preparation

Mengatasi Missing Value
"""

# Membersihkan missing value dengan fungsi dropna()
books_clean = books.dropna()
books_clean

"""Memeriksa kembali missing value pada variabel books_clean"""

# Mengecek kembali missing value pada variabel books_clean
books_clean.isnull().sum()

"""Mengurutkan buku secara ascending berdasarkan nilai ISBN"""

# Mengurutkan buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_books
fix_books = books_clean.sort_values('ISBN', ascending=True)
fix_books

"""Mengecek jumlah buku pada variabel fix_books"""

# Mengecek berapa jumlah fix_books
len(fix_books.ISBN.unique())

"""Mengecek jumlah book author pada variabel fix_books"""

# Mengecek book author yang unik
fix_books['Book-Author'].unique()

"""Mengurutkan buku berdasarkan ISBN kemudian menyimpan nya ke dalam variabel preparation"""

# Membuat variabel preparation yang berisi dataframe fix_books kemudian mengurutkan berdasarkan ISBN
preparation = fix_books
preparation.sort_values('ISBN')

"""Menghapus data duplikat pada buku berdasarkan ISBN agar dapat diproses lebih lanjut untuk pemodelan"""

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

# Mengambil 8000 data secara acak agar proses perhitungan cosine similarity,
# pada matrix tf-idf di tahap modeling content based filtering berjalan dengan lancar tanpa mengalami ram crash
# pada google colab dengan ukuran dataset besar

sampled_8000_df = preparation.sample(n=8000, random_state=42)

"""Mengonversi data ISBN, Book-Title, dan Book-Author ke dalam bentuk list"""

# Mengonversi data series 'ISBN' menjadi dalam bentuk list
books_id = sampled_8000_df['ISBN'].tolist()

# Mengonversi data series 'Book-Title' menjadi dalam bentuk list
books_title = sampled_8000_df['Book-Title'].tolist()

# Mengonversi data series 'Book-Author' menjadi dalam bentuk list
books_author = sampled_8000_df['Book-Author'].tolist()

print(len(books_id))
print(len(books_title))
print(len(books_author))

"""Membuat dictionary untuk variabel books_id, books_title, dan books_author yang telah dibuat sebelumnya"""

# Membuat dictionary untuk data books_id, books_title, books_author
books_new = pd.DataFrame({
    'id': books_id,
    'book_title': books_title,
    'book_author': books_author
})
books_new

"""# Model Development dengan Content Based Filtering

Memeriksa kembali data books_new dan assign dataframe dari tahap sebelumnya ke dalam variabel data
"""

data = books_new
data.sample(5)

"""Menemukan representasi fitur penting dari setiap book author"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(data['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Melakukan fit dan transformasi ke dalam bentuk matriks"""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Menghasilkan vektor tf-idf dalam bentuk matriks"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan book_author
# Baris diisi dengan book_title

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.book_title
).sample(22, axis=1).sample(10, axis=0)

"""Menghitung derajat kesamaan (similarity degree) antar buku dengan teknik cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Melihat matriks kesamaan setiap buku dengan menampilkan book_title dalam 5 sampel kolom dan 10 sampel baris"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa book_title
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Membuat fungsi books_recommendations dengan beberapa parameter seperti judul_buku, similarity_data, items dan k"""

def books_recommendations(judul_buku, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    judul_buku : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop judul_buku agar judul buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Melihat detail informasi id, book_title, dan book_author pada sebuah buku"""

data[data.book_title.eq('The age of innocence')]

# Mendapatkan rekomendasi buku yang mirip dengan The age of innocence
books_recommendations('The age of innocence')

"""Melakukan evaluasi metrik dengan precision pada sistem rekomendasi Content Based Filtering, hal ini dilakukan untuk memeriksa ada berapa banyak atau persentase konten relevan (similiar) yang direkomendasikan"""

precision = (2 / 5) * 100
print('Precision: ', precision, '%')

"""# Model Development dengan Collaborative Filtering

Mengambil kolom User-ID, ISBN, Book-Rating pada dataframe preparation untuk disimpan di variabel baru df
"""

rating = ['User-ID', 'ISBN', 'Book-Rating']

df = preparation[rating]
df.head()

"""## Data Preparation

Melakukan persiapan data untuk menyandikan (encode) fitur ‘User-ID’ ke dalam indeks integer
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)

# Melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

"""Melakukan persiapan data untuk menyandikan (encode) fitur ‘ISBN’ ke dalam indeks integer"""

# Mengubah ISBN menjadi list tanpa nilai yang sama
books_ids = df['ISBN'].unique().tolist()

# Melakukan proses encoding ISBN
books_to_books_encoded = {x: i for i, x in enumerate(books_ids)}

# Melakukan proses encoding angka ke ISBN
books_encoded_to_books = {i: x for i, x in enumerate(books_ids)}

"""Memetakan User-ID dan ISBN ke dataframe yang berkaitan."""

# Mapping User-ID ke dataframe users
df['user'] = df['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe books
df['book'] = df['ISBN'].map(books_to_books_encoded)

"""Memeriksa beberapa hal dalam data seperti jumlah user, jumlah buku, dan mengubah nilai rating menjadi float"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_books = len(books_encoded_to_books)
print(num_books)

# Mengubah rating menjadi nilai float
df['rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_books, min_rating, max_rating
))

"""Melakukan pembagian data menjadi data training dan validasi. Sebelum itu, mengacak datanya terlebih dahulu agar distribusinya menjadi random"""

# Mengacak dataset dan hanya mengambil 10000 data secara acak
df = df.sample(n=10000, random_state=42)
df

"""Membagi data train dan validasi dengan komposisi 80:20. kemudian, memetakan (mapping) data user dan book menjadi satu value terlebih dahulu serta membuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Membuat class RecommenderNet dengan keras Model class"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.books_embedding = layers.Embedding( # layer embeddings books
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.books_bias = layers.Embedding(num_books, 1) # layer embedding books bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    books_vector = self.books_embedding(inputs[:, 1]) # memanggil layer embedding 3
    books_bias = self.books_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Melakukan proses compile terhadap model"""

model = RecommenderNet(num_users, num_books, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Melakukan proses training model"""

# Memulai training
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 12,
    epochs = 25,
    validation_data = (x_val, y_val)
)

"""Melihat visualisasi proses training"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Mendapatkan data buku yang belum pernah dibaca oleh pengguna kemudian menyimpannya ke dalam variabel books_not_visited sebagai daftar buku untuk direkomendasikan pada pengguna"""

books_df = books_new

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
books_visited_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
books_not_visited = books_df[~books_df['id'].isin(books_visited_by_user.ISBN.values)]['id']
books_not_visited = list(
    set(books_not_visited)
    .intersection(set(books_to_books_encoded.keys()))
)

books_not_visited = [[books_to_books_encoded.get(x)] for x in books_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_books_array = np.hstack(
    ([[user_encoder]] * len(books_not_visited), books_not_visited)
)

"""Memperoleh rekomendasi buku menggunakan fungsi model.predict()"""

ratings = model.predict(user_books_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_books_ids = [
    books_encoded_to_books.get(books_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)

top_books_user = (
    books_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

books_df_rows = books_df[books_df['id'].isin(top_books_user)]
for row in books_df_rows.itertuples():
    print(row.book_title, ':', row.book_author)

print('----' * 8)
print('Top 10 books recommendation')
print('----' * 8)

recommended_books = books_df[books_df['id'].isin(recommended_books_ids)]
for row in recommended_books.itertuples():
    print(row.book_title, ':', row.book_author)

